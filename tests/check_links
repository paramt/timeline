#!/usr/bin/env python
# -*- coding: UTF-8 -*-

# List of URLs to skip over
blacklisted = ["https://go.param.me/spellbind", "https://web.archive.org/web/20170204071134/http://appinventor.mit.edu/explore/app-month-gallery.html", "https://www.instagram.com/p/CCrhvqqMUNz"]

import csv
import requests
from urlextract import URLExtract

markdown_file = "README.md"
csv_file = "_data/timeline.csv"
links = []
exit_status = 0

print(f"Collecting URLs from {csv_file}")

with open(csv_file) as f:
    events = []
    lines = f.readlines()

    for l in  csv.reader(lines, quotechar='"', delimiter=',', skipinitialspace=True):
        events.append(l[2])

    events.pop(0)

extractor = URLExtract()
links += extractor.find_urls(" ".join(events))

print(f"Collecting URLs from {markdown_file}")

with open(markdown_file) as f:
    text = f.read()

extractor = URLExtract()
markdown = extractor.find_urls(text)

links += markdown

print("Cleaning up list of URLs")

# Remove blacklisted links
for link in links:
    if link in blacklisted:
        links.remove(link)
        print(f"Removed {link}")

print("Checking all URLs")

for url in links:
    try:
        request = requests.get(url)
        if request.status_code == 200:
            print(f"✓ 200 {url}")
        elif request.status_code >= 400:
            print(f"✕ {request.status_code} {url}")
            exit_status = 1
        else:
            print(f"⚪ {request.status_code} {url}")

    except:
        print(f"✕ ERR {url}")

        # Continue through all URLs but fail test at the end
        exit_status = 1
        continue

exit(exit_status)
